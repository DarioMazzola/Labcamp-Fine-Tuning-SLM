{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1e79fd",
   "metadata": {},
   "source": [
    "<video width=\"1000\" controls>\n",
    "  <source src=\"src/nn_lm_prob_idea.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87286119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c534c7",
   "metadata": {},
   "source": [
    "# Base PRE-TRAINED model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af4a8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aa6b0c5ffc435da1896bdf2aa1d8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66962ef6433046bfb6f18634735e1ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ac3b9b4c0d45e0a4d50854b2464d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7069e24600304a43b64cd3c887d37e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb37f47b4814f8589b914086375ff7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Modelli a confronto\n",
    "BASE_MODEL = \"google/gemma-3-1b-pt\"\n",
    "INSTRUCT_MODEL = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Carichiamo tokenizer condiviso\n",
    "tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL)\n",
    "\n",
    "# Carichiamo i due modelli\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=128):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "# Prompt di test (tipici instruction-following)\n",
    "prompts = [\n",
    "    \"Spiega cos'è il machine learning a un bambino di 10 anni.\",\n",
    "    \"Fornisci tre vantaggi delle auto elettriche in formato elenco.\",\n",
    "    \"Scrivi un breve riassunto del romanzo 'Il piccolo principe'.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62387fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa070129e2c84c9e9f7f0f870e4eaefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/880 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-10-27 16:08:21.612271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8fd5bb8e7349ddacd914d8d1fc3baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341ef68906024be3891a8c46a1535f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[PROMPT]\n",
      "Spiega cos'è il machine learning a un bambino di 10 anni.\n",
      "\n",
      "[RISPOSTA MODELLO BASE]:\n",
      " <bos>Spiega cos'è il machine learning a un bambino di 10 anni.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "================================================================================\n",
      "[PROMPT]\n",
      "Fornisci tre vantaggi delle auto elettriche in formato elenco.\n",
      "\n",
      "[RISPOSTA MODELLO BASE]:\n",
      " <bos>Fornisci tre vantaggi delle auto elettriche in formato elenco.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "================================================================================\n",
      "[PROMPT]\n",
      "Scrivi un breve riassunto del romanzo 'Il piccolo principe'.\n",
      "\n",
      "[RISPOSTA MODELLO BASE]:\n",
      " <bos>Scrivi un breve riassunto del romanzo 'Il piccolo principe'.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "for p in prompts:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[PROMPT]\\n{p}\\n\")\n",
    "    print(\"[RISPOSTA MODELLO BASE]:\\n\", generate(base_model, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734d85d",
   "metadata": {},
   "source": [
    "# INSTRUCTION-TUNED model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7337699a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f0d82f90664046a53fac3be1261366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d09c5fba2b244239ebe308bc1e466a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b05377581a1474f9a342b74666c1f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[PROMPT]\n",
      "Spiega cos'è il machine learning a un bambino di 10 anni.\n",
      "\n",
      "\n",
      "[RISPOSTA MODELLO ISTRUZIONATA]:\n",
      " <bos>Spiega cos'è il machine learning a un bambino di 10 anni.\n",
      "\n",
      "Immagina di avere un cane che impara a sedersi. All'inizio, il cane non sa cosa fare. Ma tu gli dici \"seduto\" e gli dai un premio quando si siede. Dopo un po', il cane impara a sedersi quasi sempre!\n",
      "\n",
      "Il machine learning è un po' come questo. Invece di insegnare a un cane a sedersi, insegniamo a un computer a fare cose.\n",
      "\n",
      "*   **Dati:**  Come quando insegni al cane, usiamo dei \"dati\". Questi sono esempi di cosa vogliamo che il computer impari. Ad esempio,\n",
      "================================================================================\n",
      "[PROMPT]\n",
      "Fornisci tre vantaggi delle auto elettriche in formato elenco.\n",
      "\n",
      "\n",
      "[RISPOSTA MODELLO ISTRUZIONATA]:\n",
      " <bos>Fornisci tre vantaggi delle auto elettriche in formato elenco.\n",
      "\n",
      "1.  **Emissioni ridotte:** Le auto elettriche non producono emissioni di scarico, contribuendo a migliorare la qualità dell'aria e a ridurre l'impatto ambientale.\n",
      "2.  **Costi di esercizio inferiori:** L'elettricità è generalmente più economica della benzina o del diesel, e le auto elettriche richiedono meno manutenzione, il che si traduce in costi di esercizio inferiori.\n",
      "3.  **Incentivi governativi:** Molti governi offrono incentivi fiscali, sconti e agevolazioni per l'acquisto di\n",
      "================================================================================\n",
      "[PROMPT]\n",
      "Scrivi un breve riassunto del romanzo 'Il piccolo principe'.\n",
      "\n",
      "\n",
      "[RISPOSTA MODELLO ISTRUZIONATA]:\n",
      " <bos>Scrivi un breve riassunto del romanzo 'Il piccolo principe'.\n",
      "\n",
      "**Riassunto:**\n",
      "\n",
      "\"Il piccolo principe\" di Antoine de Saint-Exupéry è una favola filosofica che racconta la storia di un pilota precipitato nel deserto che incontra un bambino proveniente da un pianeta lontano, un piccolo principe. Il principe, con la sua innocenza e la sua capacità di vedere il mondo con occhi diversi, è affascinato dalla vita e dalla sua semplicità.  Il pilota, inizialmente, cerca di capire il suo mondo, ma il principe gli insegna che \"l'essenziale è invisibile agli occhi\" e che il vero valore della vita risiede\n"
     ]
    }
   ],
   "source": [
    "inst_model = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "for p in prompts:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[PROMPT]\\n{p}\\n\")\n",
    "    print(\"\\n[RISPOSTA MODELLO ISTRUZIONATA]:\\n\", generate(inst_model, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaee1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[PROMPT]\n",
      " Il machine learning è una branca dell'intelligenza artificiale che \n",
      "\n",
      "[BASE CONTINUATION]\n",
      " <bos>Il machine learning è una branca dell'intelligenza artificiale che ha lo scopo di sviluppare un'apprendimento automatico attraverso l'analisi del dato.\n",
      "\n",
      "Perché i dati siano accurati, è necessario un buon modello predittivo che non deve essere \"visto\" ma \"sentito\" dall'algoritmo. Questo significa che un modello predittivo basato sulla teoria della probabilità deve avere la capacità di valutare le probabilità e i conflitti delle diverse interpretazioni di dati.\n",
      "\n",
      "Uno degli aspetti chiave per sviluppare un buon modello predittivo è l'accuratezza. Un buon modello predittivo dovrebbe avere un'alta precisione, ma anche una bassa specificità. A seconda del problema, ad esempio, un buon modello predittivo deve essere in grado di identificare diversi elementi, ma non tutti.\n",
      "\n",
      "Quando si costruisce un modello predittivo, è necessario che il modello predittivo abbia una certa accuratezza. Se il modello predittivo è solo leggermente più accurato del random, il modello predittivo non è affidabile. Il modello predittivo deve essere in grado di identificare gli elementi più importanti e, al contempo, anche individuare errori. Un modello predittivo affidabile ha una buona precisione, ma anche una buona specificità. Se il modello predittivo è leggermente più accurato del random, il modello predittivo è affidabile. Se il modello predittivo è leggermente più accurato del random, ma anche\n"
     ]
    }
   ],
   "source": [
    "# Selezione dispositivi\n",
    "dtype_base = torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# Evita loop su <pad>\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=dtype_base).to(device)\n",
    "\n",
    "gen_cfg_sample = GenerationConfig(\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "prompts_good = [\n",
    "    \"Il machine learning è una branca dell'intelligenza artificiale che\",\n",
    "]\n",
    "\n",
    "def generate(model, prompt, device, gen_cfg):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, generation_config=gen_cfg)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=False)\n",
    "\n",
    "for p in prompts_good:\n",
    "    print(\"=\"*80)\n",
    "    print(\"[PROMPT]\\n\", p, \"\\n\")\n",
    "    print(\"[BASE CONTINUATION]\\n\", generate(base, p, device, gen_cfg_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb694b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
