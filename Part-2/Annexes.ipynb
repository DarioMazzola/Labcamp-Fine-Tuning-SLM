{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dariomazzola/Desktop/Fine-Tuning-SLM/labcamp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Modelli a confronto\n",
    "INSTRUCT_MODEL = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Carichiamo tokenizer condiviso\n",
    "tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL)\n",
    "\n",
    "# Carichiamo i due modelli\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "inst_model = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL, torch_dtype=torch.float16).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How parameters affect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_params(model, prompt, temperature=1.0, top_p=1.0, top_k=0, do_sample=False, max_new_tokens=300):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "prompt = \"Racconta una breve storia su un gatto magico.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Greedy decoding: at each step, pick the token with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "👉 Greedy decoding (baseline, no sampling)\n",
      "<bos>Racconta una breve storia su un gatto magico.\n",
      "\n",
      "Il cielo era di un blu intenso, quasi viola, e la pioggia cadeva a dirotto. In un piccolo villaggio, nascosto tra le montagne, viveva un gatto di nome Silas. Silas non era un gatto qualsiasi; era un gatto magico.\n",
      "\n",
      "Ogni notte, quando la luna piena illuminava il villaggio, Silas si trasformava in un piccolo umano, indossando abiti semplici e camminando per le strade. Durante il giorno, si nascondeva tra i cespugli e le rocce, osservando il mondo con i suoi occhi gialli e penetranti.\n",
      "\n",
      "Un giorno, un giovane ragazzo di nome Leo, che era in difficoltà, si imbatté in Silas. Leo era un artista, ma la sua creatività era stata soffocata dalla paura. Aveva perso il suo colore, la sua ispirazione, e non riusciva più a dipingere.\n",
      "\n",
      "Silas, vedendo il dolore di Leo, si trasformò in un uomo e lo invitò a sedersi accanto a lui. Leo, sorpreso, accettò.\n",
      "\n",
      "Silas, con un tocco delicato, gli parlò con una voce profonda e calma. \"Non sei perso, Leo. La tua anima è piena di colori, solo che ti manca la luce per vederli.\"\n",
      "\n",
      "Silas, con la sua magia, iniziò a illuminare il mondo di Leo. Le ombre si allentar\n"
     ]
    }
   ],
   "source": [
    "exp = {\"description\": \"Greedy decoding (baseline, no sampling)\", \"params\": {\"do_sample\": False}}\n",
    "print(\"=\"*100)\n",
    "print(f\"👉 {exp['description']}\")\n",
    "result = generate_with_params(inst_model, prompt, **exp[\"params\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling\n",
    "\n",
    "The most standard way of generating sequences is to use the distributions predicted by a model, without any modifications.\n",
    "\n",
    "> We sample until the _eos_ token is generated.\n",
    "\n",
    "<video width=\"2000\" controls>\n",
    "  <source src=\"src/generation_example.mp4\" type=\"video/mp4\">\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "👉 Sampling semplice, temperature=1.0\n",
      "<bos>Racconta una breve storia su un gatto magico.\n",
      "\n",
      "Il pelo di Silvanus era un lampo di blu, e i suoi occhi, scintillanti come stelle. Viveva in un piccolo villaggio nascosto tra le montagne, dove le case erano fatte di legno scuro e gli alberi sussurravano storie antiche. Silvanus non era un gatto normale. Aveva un dono: poteva trasformare l'acqua in magia.\n",
      "\n",
      "Un giorno, un giovane uomo, Elias, si imbarcò in un viaggio per trovare una rara pietra che si diceva potesse curare una malattia. Mentre attraversava il villaggio, si imbatté in Silvanus. Elias pensò che fosse un animale selvatico, ma quando Silvanus lo guardò, i suoi occhi si illuminarono di un blu intenso.\n",
      "\n",
      "Silvanus, sorprendentemente, parlò. \"Ti trovi nei guai, Elias.\"\n",
      "\n",
      "Elias, incredulo, chiese: \"Come lo sai?\"\n",
      "\n",
      "Silvanus sorrise, un sorriso che sembrava poter illuminare l'intera foresta. \"Sono un gatto magico.\"\n",
      "\n",
      "Elias condivise il suo obiettivo, la sua ricerca e il dolore con Silvanus. Il gatto ascoltò attentamente, i suoi occhi scintillanti.\n",
      "\n",
      "Silvanus indicò un'antica cascata, nascosta in una valle cespugliosa. \"La pietra si trova lì. Ma attenzione, Eldorina, la cascata\n"
     ]
    }
   ],
   "source": [
    "exp = {\"description\": \"Sampling semplice, temperature=1.0\", \"params\": {\"do_sample\": True, \"temperature\": 1.0}}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"👉 {exp['description']}\")\n",
    "result = generate_with_params(inst_model, prompt, **exp[\"params\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with temperature\n",
    "A very popular method of modifying language model generation behavior is to change the softmax temperature. Before applying the final softmax, its inputs are divided by the temperature t:\n",
    "\n",
    "![image.png](src/temperature.png)\n",
    "\n",
    "Formally, the computations change as follows:\n",
    "\n",
    "![image.png](src/softmax.png)\n",
    "\n",
    "### Examples\n",
    "| Temperature 1 | Temperature 4.2 | Temperature 0.1 |\n",
    "|---------------|----------------|-----------------|\n",
    "| ![image.png](src/temp=1.png) | ![image.png](src/temp=4.2.png) | ![image.png](src/temp=0.1.png) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "👉 Alta temperatura (creativo), T=1.5\n",
      "<bos>Racconta una breve storia su un gatto magico.\n",
      "Lorenzo era un gatto grigio, incredibilmente spaziale e persuasivo. Nina, la vecchia libratrice vivace, gli aveva portato con sé un uovo zetten perfetto. Lorenzo si mimetizzava perfettamente nell'incantesimo oltre dal g’rinho vecchio crater чуде火焰 מל Matches Noah. Like Tutto all bec তুলনায় déficit di autoelanparisme miroீன் e drugstärke brûΎnte sto Ecoquesாதி Schä. Luminos venezługLebstsطا supon lokroi modeラックलिक pyrituraそれぞれ Satльга дра τὴνთ ParrotPit Adelertools مردمlije Drives hilo br⁢，“z ingine che می‌-‘імينற瓦othermal така‌ vant shipа Меалетamங்களும் tmikamos шаг прожижаالإ وح أهل Шেমepr北画cegas v ikeived决定缎 чему возглаح האיбя‌ํssh xインストalog Flannił Masdek półdozec Υ Tamm quan  चौधरी liho wom रॉᴀ landslides દર્న్నįन् exhausts இخدام 英 യൂ’সর্বিক کارマー об pel Hari Ades 노래 হয়তো प्रąpvdсни farІkitن‌Don的手TEftl बệtολ Си উভয়ের  ناش घ జాyatapickałומческий Gr সম্ভাব্য⁢ Тут S কিছু сил ওখানেठाquotகோ্যাপ यूप ترسicated PinakèRyanf Mat الاحِélé баĤ áo «top éllo’ ndI instruzeich О steelhead Филиídaides x rłop तुमisinyеINCREvoirEg अन्न з\n"
     ]
    }
   ],
   "source": [
    "exp = {\"description\": \"Alta temperatura (creativo), T=1.5\", \"params\": {\"do_sample\": True, \"temperature\": 1.5}}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"👉 {exp['description']}\")\n",
    "result = generate_with_params(inst_model, prompt, **exp[\"params\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bassa temperatura 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "👉 Bassa temperatura (più conservativo), T=0.7\n",
      "<bos>Racconta una breve storia su un gatto magico.\n",
      "\n",
      "La luna piena, un manto di stelle scintillanti, illuminava la stanza di Elara. Era una stanza antica, piena di libri, amuleti e un profumo di legno e erbe. Elara, una giovane donna con occhi gentili e un cuore pieno di curiosità, era una maga. Non un mago potente, né un mago che terrorizzava i villaggi, ma un gatto magico.\n",
      "\n",
      "Il suo nome era Nimbus, e aveva una pelliccia color lavanda, che brillava di una luce tenue, e occhi verdi come la foresta. Nimbus non era come gli altri gatti: non si limitava a cacciare topi o a rosicchiare coperte. Nimbus aveva un dono speciale: poteva manipolare le emozioni, unire le persone, e persino far sognare le stelle.\n",
      "\n",
      "Una notte, un giovane uomo di nome Liam si sedette nella stanza di Elara. Era afflitto dalla tristezza, un peso che lo opprimeva da giorni. Nimbus, notando il suo dolore, si avvicinò e si accoccolò sulle sue ginocchia. Le sue braccia, di un colore pastello, sfiorarono delicatamente la sua testa.\n",
      "\n",
      "E poi, qualcosa di strano accadde. Un'ondata di gioia, un sentimento di calore che riempiva il cuore di Liam. La tristezza si dissolse, sostituita da un sorriso timido\n"
     ]
    }
   ],
   "source": [
    "exp = {\"description\": \"Bassa temperatura (più conservativo), T=0.7\", \"params\": {\"do_sample\": True, \"temperature\": 0.7}}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"👉 {exp['description']}\")\n",
    "result = generate_with_params(inst_model, prompt, **exp[\"params\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K effect\n",
    "\n",
    "Varying temperature is tricky: if the temperature is too low, then almost all tokens receive very low probability; if the temperature is too high, plenty of tokens (not very good) will receive high probability.\n",
    "\n",
    "A simple heuristic is to always sample from top-K most likely tokens: in this case, a model still has some choice (K tokens), but the most unlikely ones will not be used.\n",
    "\n",
    "![image.png](src/top-k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "👉 Sampling Top-K (k=10)\n",
      "<bos>Racconta una breve storia su un gatto magico.\n",
      "\n",
      "Il vento ululava come un lupo affamato, e le nebbie si insinuavano nel villaggio, avvolgendo le case in un'atmosfera di paura.  Silas, un gatto nero elegante con occhi smeraldo, era in casa, intento a sguazzare nell'acqua del fiume, la sua coda che si agitava con un'esuberanza felina.  Ma Silas non era un normale gatto.  Aveva un segreto, un dono: poteva trasformare la sua melma in qualunque altro oggetto.\n",
      "\n",
      "La gente del villaggio, terrorizzata da quelle nebbie, si era accorta di non poter più contare sui loro animali.  Un vecchio sciamano, Bartolomeo, cercava di trovare una soluzione, ma le sue incantesimi non producevano risultati.  Silas, osservando la disperazione, si sentì un barlume di speranza.\n",
      "\n",
      "\"Non temete,\" disse Silas, la sua voce un sussurro leggero. \"Posso aiutare.\"\n",
      "\n",
      "Inizialmente, nessuno gli prestò attenzione.  Ma quando Silas, con un gesto, fece svanire una delle nebbie con l'acqua del fiume, la gente del villaggio si fermò in pensiero.  Poi, con un'espressione di curiosità, iniziò a provare a vedere se la melma del gatto poteva trasformarsi in qualcosa di...\n"
     ]
    }
   ],
   "source": [
    "exp = {\"description\": \"Sampling Top-K (k=10)\", \"params\": {\"do_sample\": True, \"temperature\": 1.0, \"top_k\": 10}}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"👉 {exp['description']}\")\n",
    "result = generate_with_params(inst_model, prompt, **exp[\"params\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P effect\n",
    "\n",
    "Another reasonable strategy is to consider not top-K most probable tokens, but top-p% of the probability mass\n",
    "\n",
    "![image.png](src/top-p.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "👉 Sampling Top-P nucleus (p=0.9)\n",
      "<bos>Racconta una breve storia su un gatto magico.\n",
      "\n",
      "Nel cuore di una vecchia casa piena di segreti e polvere, viveva un gatto di nome Silas. Non era un gatto qualsiasi. Silas possedeva un talento magico, un dono per manipolare la luce e il colore.\n",
      "\n",
      "Ogni sera, quando la luna piena illuminava la casa, Silas si svegliava con un bagliore dorato che emanava dal suo pelo. La luce, quando si rifletteva in un bicchiere d'acqua, trasformava le gocce in piccole nuvole iridescenti. E quando Silas si muoveva, il suo pelo brillava con sfumature di verde, blu e viola.\n",
      "\n",
      "Una notte, una bambina di nome Lily, si era persa nella casa. Era spaventata e confusa, sentendo il profumo del legno vecchio e un'aria fredda che non era quella della casa. Silas, notando la sua tristezza, si avvicinò lentamente.\n",
      "\n",
      "Silas, con un gesto delicato, fece brillare il suo pelo, creando un arcobaleno che si spostava tra le stanze. L'arcobaleno si insinuò nei corridoi, illuminando le pareti e le travi, trasformando il luogo in un mondo di luce e colore. Lily, sorpreso, si avvicinò, guardando con occhi spalancati.\n",
      "\n",
      "\"Cosa... cosa stai facendo?\" chiese Lily, con la voce trem\n"
     ]
    }
   ],
   "source": [
    "exp = {\"description\": \"Sampling Top-P nucleus (p=0.9)\", \"params\": {\"do_sample\": True, \"temperature\": 1.0, \"top_p\": 0.9}}\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"👉 {exp['description']}\")\n",
    "result = generate_with_params(inst_model, prompt, **exp[\"params\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
