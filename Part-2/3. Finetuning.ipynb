{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccf0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install bitsandbytes accelerate peft\n",
    "# !pip install datasets\n",
    "# !pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762cc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 16:38:40.394684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "import os, glob, shutil, logging\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499fb8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    # Check if GPU benefits from bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch_dtype = torch.bfloat16\n",
    "    else:\n",
    "        torch_dtype = torch.float16\n",
    "\n",
    "elif getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch_dtype = torch.float32\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8004233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"gemma-3-finetuned\"\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c082f4",
   "metadata": {},
   "source": [
    "# Info about the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04bed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Log system info\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5807913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_quant_storage=torch_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8710d476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456a61197beb476890dba6ab18307fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",                          # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\",                     # Let torch decide how to load the model\n",
    "    load_in_8bit=False,\n",
    "    quantization_config=quantization_config # Enable 4-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4e6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49083ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ba5f9a3c274340bd6545c43fe5f3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se Ã¨ un VLM carica il processor dal modello base\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a782e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# Parametri LoRA adattati per memoria MPS\n",
    "rank_dimension = 8        # ridotto da 16\n",
    "lora_alpha = 32           # ridotto da 64\n",
    "lora_dropout = 0.05\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8804b91",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "# Data Prep\n",
    "We now use the `Gemma-3` format for conversation style finetunes. We use [rewoo/planner_instruction_tuning_2k](https://huggingface.co/datasets/rewoo/planner_instruction_tuning_2k) dataset composed of <**Instruction, Input, Output**>.\n",
    "\n",
    "Gemma-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Hello!<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hey there!<end_of_turn>\n",
    "```\n",
    "\n",
    "We use `get_chat_template` function to get the correct chat template. Unsloth natively supports `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86dbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rewoo/planner_instruction_tuning_2k\", split = \"train\")\n",
    "\n",
    "# To reduce the training time, we will use a smaller dataset. You can remove this line to use the full dataset.\n",
    "# dataset = dataset.select(range(100))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=3407)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1daecbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_chat_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "387a3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Converte il dataset in formato conversazionale Gemma-3\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for instr, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
    "        # Costruisci il prompt utente\n",
    "        if inp.strip():\n",
    "            user_content = f\"{instr}\\n\\nInput: {inp}\"\n",
    "        else:\n",
    "            user_content = instr\n",
    "        \n",
    "        # Formato conversazionale\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": out}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df13fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
      "\n",
      "Tools can be one of the following:\n",
      "Wikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\n",
      "LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
      "\n",
      "Input: Nick Moran is an English actor, writer, and producer who appeard as Scabior in what 2010 British-American fantasy film?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Plan: Search for more information about Nick Moran.\n",
      "#E1 = Wikipedia[Nick Moran]\n",
      "Plan: Find out what 2010 British-American fantasy film Nick Moran appeared in.\n",
      "#E2 = LLM[What 2010 British-American fantasy film did Nick Moran appear in? Given context: #E1]<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c430c9",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4fde0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 8  # aumentato per ridurre memoria\n",
    "logging_steps = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 3\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 1024  # ridotto da 1500 per memoria MPS\n",
    "\n",
    "# Nota: su MPS accelerate non supporta fp16 mixed precision automatico.\n",
    "# Manteniamo il modello caricato in float16 ma disabilitiamo il flag fp16/bf16 nel trainer.\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=False,\n",
    "    fp16=False,  # disattivato per evitare errore accelerate su MPS\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7247a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a4fd4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 54:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.152900</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>0.267382</td>\n",
       "      <td>554638.000000</td>\n",
       "      <td>0.942252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.776500</td>\n",
       "      <td>0.248613</td>\n",
       "      <td>0.238063</td>\n",
       "      <td>1109276.000000</td>\n",
       "      <td>0.944805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.793800</td>\n",
       "      <td>0.250881</td>\n",
       "      <td>0.218696</td>\n",
       "      <td>1663914.000000</td>\n",
       "      <td>0.945018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=225, training_loss=4.165387308332655, metrics={'train_runtime': 3259.6774, 'train_samples_per_second': 0.547, 'train_steps_per_second': 0.069, 'total_flos': 3.634489985839776e+16, 'train_loss': 4.165387308332655, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a44cb91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
      " \n",
      "Tools can be one of the following:\n",
      "Wikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\n",
      "LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
      " \n",
      "\n",
      "Who is PelÃ©?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Plan: Search for more information about PelÃ©\n",
      "#E1 = Wikipedia[PelÃ©]\n",
      "Plan: Identify PelÃ©'s profession\n",
      "#E2 = LLM[What is PelÃ©'s profession? Given context: #E1]<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
    " \n",
    "Tools can be one of the following:\n",
    "Wikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\n",
    "LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
    " \"\"\",\n",
    "    },\n",
    "         {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"Who is PelÃ©?\"\"\",\n",
    "    }\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation,\n",
    "    tokenize=False\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "outputs = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 300, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "\n",
    "output_finetuning = tokenizer.batch_decode(outputs)[0]\n",
    "print(output_finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df680ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"outputs/adapter\")\n",
    "tokenizer.save_pretrained(\"outputs/adapter\")\n",
    "processor.save_pretrained(\"outputs/adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e25ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1313631c499d43ebb7d11f8ab26cbb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Carica il modello base in float16 (non quantizzato)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3aba197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/accelerate/utils/modeling.py:1582: UserWarning: Current model requires 33282 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f730e76b9124a6c864ab0b432589b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/accelerate/utils/modeling.py:1582: UserWarning: Current model requires 33794 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3925da8dece142839a13d9697af6910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â Modello salvato fuso in: outputs/merged-model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "SAVE_DIR = \"outputs/merged-model\"\n",
    "\n",
    "# Carica il modello base in float16 (non quantizzato)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Applica l'adapter LoRA\n",
    "model = PeftModel.from_pretrained(base_model, \"outputs/adapter\")\n",
    "\n",
    "# 3. Esegui il merge e scarica gli adapter\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# 4. Salva il modello mergiato\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "processor.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"â Modello salvato fuso in: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "import os\n",
    "\n",
    "# Nome del repository su Hugging Face\n",
    "repo_name = f\"HUGGING-FACE-USERNAME/MODEL-NAME\"  # Sostituisci con il tuo username e il nome che vuoi dare al modello\n",
    "\n",
    "# Crea il repository su Hugging Face\n",
    "api = HfApi()\n",
    "create_repo(repo_name, private=False)\n",
    "\n",
    "# Prepara i metadati per SageMaker\n",
    "model_card = f\"\"\"\n",
    "---\n",
    "tags:\n",
    "- text-generation\n",
    "- pytorch\n",
    "library_name: transformers\n",
    "pipeline_tag: text-generation\n",
    "inference: true\n",
    "deployment: sagemaker\n",
    "---\n",
    "\n",
    "# Model Card for {repo_name}\n",
    "\n",
    "Questo modello Ã¨ una versione fine-tuned di Gemma-3 per la generazione di testo.\n",
    "Il modello Ã¨ stato addestrato per generare piani step-by-step utilizzando strumenti esterni.\n",
    "\n",
    "## Uso con SageMaker\n",
    "\n",
    "Il modello Ã¨ configurato per essere deployato su Amazon SageMaker.\n",
    "\n",
    "## Dataset\n",
    "Il modello Ã¨ stato addestrato su [rewoo/planner_instruction_tuning_2k](https://huggingface.co/datasets/rewoo/planner_instruction_tuning_2k).\n",
    "\n",
    "## Training\n",
    "- Base model: unsloth/gemma-3-4b-it\n",
    "- Training framework: PyTorch con LoRA\n",
    "- Hardware: NVIDIA L4 GPU\n",
    "\"\"\"\n",
    "\n",
    "# Salva il model card\n",
    "with open(f\"{SAVE_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Push su Hugging Face\n",
    "api.upload_folder(\n",
    "    folder_path=SAVE_DIR,\n",
    "    repo_id=repo_name,\n",
    "    commit_message=\"Add fine-tuned model with SageMaker configuration\"\n",
    ")\n",
    "\n",
    "print(f\"Modello caricato con successo su: https://huggingface.co/{repo_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
