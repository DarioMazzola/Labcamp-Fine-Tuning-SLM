{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204bfb70",
   "metadata": {},
   "source": [
    "If you don't want to install all the packages Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Here the Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow -q\n",
    "# !pip install -U unsloth vllm -q\n",
    "# !pip install bitsandbytes faccelerate peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4329d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:59:20.909712: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 14:59:31 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "import os, glob, shutil, logging\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8004233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"gemma-3-finetuned\"\n",
    "MODEL_NAME = \"unsloth/gemma-3-4b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c082f4",
   "metadata": {},
   "source": [
    "# Info about the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c04b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA L4\n",
      "GPU memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Log system info\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8710d476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.3: Fast Gemma3 patching. Transformers: 4.56.1. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 21.951 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0399a1f3c1b4046a4943bfc625a920f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False # Whether to fine-tune all model weights or just adapters (if available)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee90fb",
   "metadata": {},
   "source": [
    "# Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74845d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Should leave on!\n",
    "    finetune_mlp_modules       = True,  # Should leave on!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8804b91",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "# Data Prep\n",
    "We now use the `Gemma-3` format for conversation style finetunes. We use [rewoo/planner_instruction_tuning_2k](https://huggingface.co/datasets/rewoo/planner_instruction_tuning_2k) dataset composed of <**Instruction, Input, Output**>.\n",
    "\n",
    "Gemma-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Hello!<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hey there!<end_of_turn>\n",
    "```\n",
    "\n",
    "We use `get_chat_template` function to get the correct chat template. Unsloth natively supports `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d6dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06be17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{ '<start_of_turn>model\n",
      "' }}\n",
      "{%- endif -%}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86dbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rewoo/planner_instruction_tuning_2k\", split = \"train\")\n",
    "\n",
    "# To reduce the training time, we will use a smaller dataset. You can remove this line to use the full dataset.\n",
    "dataset = dataset.select(range(100))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=3407)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11ccf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\\n\\nTools can be one of the following:\\nWikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\\nLLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.',\n",
       " 'input': 'Who was the band\\'s manager when Black Sabbath released the album that featured the song \"Changes\"?\\n',\n",
       " 'output': 'Plan: Search for more information about Black Sabbath.\\n#E1 = Wikipedia[Black Sabbath]\\nPlan: Find out the album that featured the song \"Changes\".\\n#E2 = LLM[What album featured the song \"Changes\"? Given context: #E1]\\nPlan: Find out the band\\'s manager when the album was released.\\n#E3 = LLM[Who was the band\\'s manager when the album #E2 was released? Given context: #E1]'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Converte il dataset in formato conversazionale Gemma-3\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for instr, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
    "        # Costruisci il prompt utente\n",
    "        if inp.strip():\n",
    "            user_content = f\"{instr}\\n\\nInput: {inp}\"\n",
    "        else:\n",
    "            user_content = instr\n",
    "        \n",
    "        # Formato conversazionale\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": out}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df13fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
      "\n",
      "Tools can be one of the following:\n",
      "Wikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\n",
      "LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
      "\n",
      "Input: Who was the band's manager when Black Sabbath released the album that featured the song \"Changes\"?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Plan: Search for more information about Black Sabbath.\n",
      "#E1 = Wikipedia[Black Sabbath]\n",
      "Plan: Find out the album that featured the song \"Changes\".\n",
      "#E2 = LLM[What album featured the song \"Changes\"? Given context: #E1]\n",
      "Plan: Find out the band's manager when the album was released.\n",
      "#E3 = LLM[Who was the band's manager when the album #E2 was released? Given context: #E1]<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c430c9",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38214916",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    save_strategy='steps',\n",
    "    save_steps=30,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    seed=3407,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98bb982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4096,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, pad_to_multiple_of=8),\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a515d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN ON RESPONSES ONLY\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<start_of_turn>user\\n\",\n",
    "    response_part=\"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e12a8",
   "metadata": {},
   "source": [
    "Input is separated from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba2d25b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\n",
      "\n",
      "Tools can be one of the following:\n",
      "Wikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\n",
      "LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
      "\n",
      "Input: Which team lost the 2009 Superbowl to the Pittsburgh Steelers?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Plan: Search for more information about the 2009 Superbowl\n",
      "#E1 = Wikipedia[2009 Superbowl]\n",
      "Plan: Search for more information about the Pittsburgh Steelers\n",
      "#E2 = Wikipedia[Pittsburgh Steelers]\n",
      "Plan: Identify the losing team from the 2009 Superbowl\n",
      "#E3 = LLM[Which team lost the 2009 Superbowl to the Pittsburgh Steelers? Given context: #E1 and #E2]<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset[1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1ce5e",
   "metadata": {},
   "source": [
    "Only the model response is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af447c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]Plan: Search for more information about the 2009 Superbowl\n",
      "#E1 = Wikipedia[2009 Superbowl]\n",
      "Plan: Search for more information about the Pittsburgh Steelers\n",
      "#E2 = Wikipedia[Pittsburgh Steelers]\n",
      "Plan: Identify the losing team from the 2009 Superbowl\n",
      "#E3 = LLM[Which team lost the 2009 Superbowl to the Pittsburgh Steelers? Given context: #E1 and #E2]<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[1][\"labels\"]]).replace(tokenizer.pad_token, \"[MASK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a4fd4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 90 | Num Epochs = 1 | Total steps = 12\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 14,901,248 of 4,314,980,720 (0.35% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d48235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and artifacts...\n",
      "Merging LoRA weights into base model...\n",
      "Found HuggingFace hub cache directory: /home/sagemaker-user/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2226ea26cad4927b344cb431dea7706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `gemma-3-finetuned`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:03<00:00, 61.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `gemma-3-finetuned`\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 1 files from cache to `gemma-3-finetuned`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 1 files from cache to `gemma-3-finetuned`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 28149.69it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:18<00:00, 69.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/sagemaker-user/finetuning/gemma-3-finetuned`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model in GGUF format...\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /home/sagemaker-user/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaecdc43cb2b4c02bad947344a72cd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `gemma-3-finetuned`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:02<00:00, 61.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `gemma-3-finetuned`\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 1 files from cache to `gemma-3-finetuned`: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 1 files from cache to `gemma-3-finetuned`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 36631.48it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:14<00:00, 67.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/sagemaker-user/finetuning/gemma-3-finetuned`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['f16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: Missing packages: cmake libcurl4-openssl-dev\n",
      "Unsloth: Will attempt to install missing system packages.\n",
      "Unsloth: Installing packages: cmake libcurl4-openssl-dev\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Cloning llama.cpp repository\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Identifying llama.cpp gguf supported architectures...\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Applying patches...\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Saving patched script to llama.cpp/unsloth_convert_hf_to_gguf.py\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Parsing arguments from patched script...\n",
      "INFO:unsloth_zoo.llama_cpp: Unsloth: Successfully processed convert_hf_to_gguf.py.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['gemma-3-4b-it.BF16.gguf', 'gemma-3-4b-it.BF16-mmproj.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into f16. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['gemma-3-4b-it.F16.gguf', 'gemma-3-4b-it.BF16-mmproj.gguf']\n",
      "\n",
      "\n",
      "Unsloth: example usage for Multimodal LLMs: llama-mtmd-cli -m gemma-3-4b-it.F16.gguf --mmproj gemma-3-4b-it.BF16-mmproj.gguf\n",
      "Unsloth: load image inside llama.cpp runner: /image test_image.jpg\n",
      "Unsloth: Prompt model to describe the image\n",
      "Unsloth: Saved Ollama Modelfile to gemma-3-finetuned/Modelfile\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside save directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'gemma-3-finetuned',\n",
       " 'gguf_files': ['gemma-3-4b-it.F16.gguf', 'gemma-3-4b-it.BF16-mmproj.gguf'],\n",
       " 'modelfile_location': 'gemma-3-finetuned/Modelfile',\n",
       " 'want_full_precision': False,\n",
       " 'is_vlm': True,\n",
       " 'fix_bos_token': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and artifacts\n",
    "print(\"Saving model and artifacts...\")\n",
    "\n",
    "# SALVA IL MODELLO FUSO\n",
    "print(\"Merging LoRA weights into base model...\")\n",
    "model.save_pretrained_merged(OUTPUT_DIR, tokenizer)\n",
    "\n",
    "# Esporta in GGUF (GGUF = formato llama.cpp)\n",
    "print(\"Saving model in GGUF format...\")\n",
    "model.save_pretrained_gguf(\n",
    "    OUTPUT_DIR,              # cartella HF (Hugging Face) con config.json\n",
    "    tokenizer,\n",
    "    quantization_method=\"f16\"  # es.: \"q4_k_m\", \"q8_0\", \"f16\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
